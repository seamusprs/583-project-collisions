{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import altair as alt\n",
    "import scipy.interpolate as interpolate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"switrs.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM collisions WHERE county_location = 'los angeles'\n",
    "    \"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, con, parse_dates = [\"collision_date\"])\n",
    "df[\"year\"] = df[\"collision_date\"].dt.year\n",
    "df[\"hour\"] = pd.to_datetime(df[\"collision_time\"]).dt.hour\n",
    "df = df.query(\"year < 2021\") # remove incomplete 2021 data\n",
    "df[\"alcohol_involved\"] = df[\"alcohol_involved\"].fillna(0) # convert NaN to 0 in alcohol use column\n",
    "\n",
    "dfc = df[[\"case_id\", \"county_location\", \"alcohol_involved\", \"collision_severity\", \"injured_victims\", \"collision_date\", \"year\", \"collision_time\", \"hour\", \"party_count\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of the proportion of crashes involving alcohol by time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will attempt to use multiple models to fit this distribution. We will use polynomial splines and B-splines but it appears a skewed-normal may be a possible fit as well.\n",
    "\n",
    "x = dfc.groupby(\"hour\")[\"hour\"].mean()\n",
    "ya = dfc[dfc[\"alcohol_involved\"] == 1].groupby(\"hour\")[\"alcohol_involved\"].count() / len(dfc[dfc[\"alcohol_involved\"] == 1])\n",
    "yn = dfc[dfc[\"alcohol_involved\"] == 0].groupby(\"hour\")[\"alcohol_involved\"].count() / len(dfc[dfc[\"alcohol_involved\"] == 0])\n",
    "adj_hour = [16, 17, 18, 19, 20, 21, 22, 23, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "adjlabel = \"{0: '8am', 1: '9am', 2: '10am', 3: '11am', 4: '12pm', 5: '1pm', 6: '2pm', 7: '3pm', 8: '4pm', 9: '5pm', 10: '6pm', 11: '7pm', 12: '8pm', 13: '9pm', 14: '10pm', 15: '11pm', 16: '12am', 17: '1am', 18: '2am', 19: '3am', 20: '4am', 21: '5am', 22: '6am', 23: '7am'}[datum.value]\"\n",
    "\n",
    "dfah = pd.DataFrame({\"hour\": x, \"adj_hour\": adj_hour, \"collisions_a\": ya, \"collisions_n\": yn})\n",
    "\n",
    "pla = alt.Chart(dfah).mark_bar(width = 20, color = \"orange\", opacity = 0.75).encode(\n",
    "    x = alt.X(\"adj_hour:O\", title = \"Hour of the day\", axis = alt.Axis(labelExpr = adjlabel)),\n",
    "    y = alt.Y(\"collisions_a\", title = \"Number of collisions (alcohol involved)\").stack(None)\n",
    ").properties(width = 600, height = 200)\n",
    "\n",
    "pln = alt.Chart(dfah).mark_bar(width = 20, opacity = 0.75).encode(\n",
    "    x = alt.X(\"adj_hour:O\", title = \"Hour of the day\", axis = alt.Axis(labelExpr = adjlabel)),\n",
    "    y = alt.Y(\"collisions_n\", title = \"Number of collisions (alcohol not involved)\").stack(None)\n",
    ").properties(width = 600, height = 200)\n",
    "\n",
    "(pln + pla).resolve_axis(y = \"independent\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dfc.groupby(\"hour\")[\"hour\"].mean()\n",
    "y = dfc.groupby(\"hour\")[\"alcohol_involved\"].mean()\n",
    "xx = np.linspace(x.min(), x.max(), 24)\n",
    "\n",
    "t, c, k = interpolate.splrep(x, y, k = 3, s = 0.0001)\n",
    "ypred = interpolate.splev(xx, (t, c, k))\n",
    "\n",
    "adj_hour = [16, 17, 18, 19, 20, 21, 22, 23, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "dfspl = pd.DataFrame({\"hour\": x, \"adj_hour\": adj_hour, \"p\": y, \"p_fit\": ypred})\n",
    "\n",
    "spl_rmse = mean_squared_error(dfspl[\"p\"], dfspl[\"p_fit\"])\n",
    "print(\"Root mean squared error: %.5f\" % (np.sqrt(spl_rmse)))\n",
    "\n",
    "adjlabel = \"{0: '8am', 1: '9am', 2: '10am', 3: '11am', 4: '12pm', 5: '1pm', 6: '2pm', 7: '3pm', 8: '4pm', 9: '5pm', 10: '6pm', 11: '7pm', 12: '8pm', 13: '9pm', 14: '10pm', 15: '11pm', 16: '12am', 17: '1am', 18: '2am', 19: '3am', 20: '4am', 21: '5am', 22: '6am', 23: '7am'}[datum.value]\"\n",
    "adjscale = alt.Scale(domain = [-0.5, 23.5])\n",
    "\n",
    "spl_base = alt.Chart(dfspl).mark_bar(width = 20, color = \"orange\").encode(\n",
    "    x = alt.X(\"adj_hour\", axis = alt.Axis(labelExpr = adjlabel)),\n",
    "    y = alt.Y(\"p\")\n",
    ").properties(width = 600, height = 200)\n",
    "\n",
    "spl = alt.Chart(dfspl).mark_line(color = \"red\").encode(\n",
    "    x = alt.X(\"adj_hour\", title = \"Hour of the day\", scale = adjscale),\n",
    "    y = alt.Y(\"p_fit\", title = \"Probability of alcohol involvement\")\n",
    ").properties(width = 600, height = 200)\n",
    "\n",
    "(spl_base + spl).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions of the number of injuries per collision, alcohol vs. no alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These appear to follow exponential or Weibull distributions. We will determine which is the best fit and then compare to see if alcohol has an effect on the distribution.\n",
    "\n",
    "# Group by 'alcohol_involved' and 'injured_victims' to count the number of accidents with each number of injuries\n",
    "injury_distribution = dfc.groupby(['alcohol_involved', 'injured_victims']).size().reset_index(name='accident_count')\n",
    "\n",
    "# Calculate the total number of accidents for each alcohol involvement category\n",
    "total_accidents_by_alcohol = injury_distribution.groupby('alcohol_involved')['accident_count'].transform('sum')\n",
    "\n",
    "# Calculate the proportion of each accident count within the alcohol involvement category\n",
    "injury_distribution['proportion'] = injury_distribution['accident_count'] / total_accidents_by_alcohol\n",
    "\n",
    "# Create the Altair chart\n",
    "chart = alt.Chart(injury_distribution).mark_bar().encode(\n",
    "    x=alt.X('injured_victims:O', title='Number of Injuries'),\n",
    "    y=alt.Y('proportion:Q', title='Proportion of Accidents'),\n",
    "    color=alt.Color('alcohol_involved:N', title='Alcohol Involved', scale=alt.Scale(domain=[0, 1], range=['red', 'blue'])),\n",
    "    column=alt.Column('alcohol_involved:N', header=alt.Header(title='Alcohol Involvement')) \n",
    ").properties(\n",
    "    width=250, \n",
    "    height=300 \n",
    ")\n",
    "\n",
    "chart.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon, weibull_min\n",
    "\n",
    "# Filter data for alcohol-involved accidents (alcohol_involved == 1)\n",
    "alcohol_df = injury_distribution[injury_distribution['alcohol_involved'] == 1]\n",
    "\n",
    "# Filter data for non-alcohol-involved accidents (alcohol_involved == 0)\n",
    "no_alcohol_df = injury_distribution[injury_distribution['alcohol_involved'] == 0]\n",
    "\n",
    "# Fit the Exponential distribution for alcohol-involved accidents\n",
    "accident_data_alcohol = alcohol_df['accident_count']\n",
    "exponential_params_alcohol = expon.fit(accident_data_alcohol)\n",
    "\n",
    "# Fit the Weibull distribution for alcohol-involved accidents\n",
    "weibull_params_alcohol = weibull_min.fit(accident_data_alcohol)\n",
    "\n",
    "# Print the parameters for alcohol-involved accidents\n",
    "print(f\"Exponential Distribution Parameters (Alcohol Involved): {exponential_params_alcohol}\")\n",
    "print(f\"Weibull Distribution Parameters (Alcohol Involved): {weibull_params_alcohol}\")\n",
    "\n",
    "# Fit the Exponential distribution for non-alcohol-involved accidents\n",
    "accident_data_no_alcohol = no_alcohol_df['accident_count']\n",
    "exponential_params_no_alcohol = expon.fit(accident_data_no_alcohol)\n",
    "\n",
    "# Fit the Weibull distribution for non-alcohol-involved accidents\n",
    "weibull_params_no_alcohol = weibull_min.fit(accident_data_no_alcohol)\n",
    "\n",
    "# Print the parameters for non-alcohol-involved accidents\n",
    "print(f\"Exponential Distribution Parameters (No Alcohol): {exponential_params_no_alcohol}\")\n",
    "print(f\"Weibull Distribution Parameters (No Alcohol): {weibull_params_no_alcohol}\")\n",
    "\n",
    "# Calculate log-likelihood for Exponential distribution for alcohol-involved accidents\n",
    "expon_loglik_alcohol = np.sum([np.log(expon.pdf(x, *exponential_params_alcohol)) for x in accident_data_alcohol])\n",
    "\n",
    "# Calculate log-likelihood for Weibull distribution for alcohol-involved accidents\n",
    "weibull_loglik_alcohol = np.sum([np.log(weibull_min.pdf(x, *weibull_params_alcohol)) for x in accident_data_alcohol])\n",
    "\n",
    "# Print log-likelihood values for alcohol-involved accidents\n",
    "print(f\"Log-Likelihood for Exponential (Alcohol Involved): {expon_loglik_alcohol}\")\n",
    "print(f\"Log-Likelihood for Weibull (Alcohol Involved): {weibull_loglik_alcohol}\")\n",
    "\n",
    "# Calculate log-likelihood for Exponential distribution for non-alcohol-involved accidents\n",
    "expon_loglik_no_alcohol = np.sum([np.log(expon.pdf(x, *exponential_params_no_alcohol)) for x in accident_data_no_alcohol])\n",
    "\n",
    "# Calculate log-likelihood for Weibull distribution for non-alcohol-involved accidents\n",
    "weibull_loglik_no_alcohol = np.sum([np.log(weibull_min.pdf(x, *weibull_params_no_alcohol)) for x in accident_data_no_alcohol])\n",
    "\n",
    "# Print log-likelihood values for non-alcohol-involved accidents\n",
    "print(f\"Log-Likelihood for Exponential (No Alcohol): {expon_loglik_no_alcohol}\")\n",
    "print(f\"Log-Likelihood for Weibull (No Alcohol): {weibull_loglik_no_alcohol}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Weibull PDF for both alcohol and no alcohol\n",
    "x_values = np.linspace(0, max(injury_distribution['injured_victims']), 100)  # Range for accident counts\n",
    "\n",
    "# Weibull PDFs for each distribution\n",
    "weibull_pdf_alcohol = weibull_min.pdf(x_values, *weibull_params_alcohol)\n",
    "weibull_pdf_no_alcohol = weibull_min.pdf(x_values, *weibull_params_no_alcohol)\n",
    "\n",
    "weibull_df = pd.DataFrame({\n",
    "    'injured_victims': x_values,\n",
    "    'weibull_pdf_alcohol': weibull_pdf_alcohol,\n",
    "    'weibull_pdf_no_alcohol': weibull_pdf_no_alcohol,\n",
    "})\n",
    "\n",
    "# Plot the Weibull PDFs\n",
    "line_chart_alcohol = alt.Chart(weibull_df).mark_line(color='blue').encode(\n",
    "    x='injured_victims:Q',\n",
    "    y='weibull_pdf_alcohol:Q'\n",
    ")\n",
    "\n",
    "line_chart_no_alcohol = alt.Chart(weibull_df).mark_line(color='red').encode(\n",
    "    x='injured_victims:Q',\n",
    "    y='weibull_pdf_no_alcohol:Q'\n",
    ")\n",
    "\n",
    "final_chart = line_chart_alcohol + line_chart_no_alcohol\n",
    "\n",
    "final_chart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a model assuming there is no difference in the distributions of alcohol vs non alcohol\n",
    "# Combine both datasets\n",
    "combined_data = np.concatenate([accident_data_alcohol, accident_data_no_alcohol])\n",
    "\n",
    "# Fit a single Weibull distribution to the combined data\n",
    "weibull_params_combined = weibull_min.fit(combined_data)\n",
    "\n",
    "# Log-likelihood for the combined data assuming same parameters for both\n",
    "log_likelihood_combined = np.sum(np.log(weibull_min.pdf(combined_data, *weibull_params_combined)))\n",
    "\n",
    "# Full model log-likelihood assuming there is a difference\n",
    "log_likelihood_full = weibull_loglik_no_alcohol + weibull_loglik_alcohol\n",
    "\n",
    "# Likelihood Ratio statistic\n",
    "lrt_statistic = -2 * (log_likelihood_combined - log_likelihood_full)\n",
    "\n",
    "df = 2 \n",
    "\n",
    "p_value = 1 - chi2.cdf(lrt_statistic, df)\n",
    "\n",
    "print(f\"LRT Statistic: {lrt_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeline for number of collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on up to 2019, predict 2020, compare to actual. This will be done using a time series model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert collision date to year_month to group by every month from 2001\n",
    "dfm = dfc.copy()\n",
    "dfm[\"year_month\"] = dfm[\"collision_date\"].dt.to_period('M') \n",
    "all_monthly_accidents = dfm.groupby('year_month').size()\n",
    "all_monthly_accidents.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sarima model \n",
    "def fit_sarima_and_forecast(time_series_data, order=(1,1,1), seasonal_order=(1,1,1,12)):\n",
    "    model = SARIMAX(time_series_data, order=order, seasonal_order=seasonal_order)\n",
    "    model_fit = model.fit(disp=False)\n",
    "    forecast = model_fit.forecast(steps=12)  \n",
    "    return forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model for all accidents to forecast 2020\n",
    "forecast_2020_monthly = fit_sarima_and_forecast(all_monthly_accidents[:'2019-12'])\n",
    "actual_2020_monthly = all_monthly_accidents['2020-01':'2020-12']\n",
    "\n",
    "#historical data upto 2019\n",
    "df = pd.DataFrame({\n",
    "    'Date': all_monthly_accidents.index,\n",
    "    'Accidents': all_monthly_accidents.values,\n",
    "    'Type': 'Historical'\n",
    "})\n",
    "\n",
    "#forecast data for 2020\n",
    "df_forecast = pd.DataFrame({\n",
    "    'Date': forecast_2020_monthly.index,\n",
    "    'Accidents': forecast_2020_monthly.values,\n",
    "    'Type': 'Forecasted'\n",
    "})\n",
    "\n",
    "#actual data for 2020\n",
    "df_actual = pd.DataFrame({\n",
    "    'Date': actual_2020_monthly.index,\n",
    "    'Accidents': actual_2020_monthly.values,\n",
    "    'Type': 'Actual'\n",
    "})\n",
    "\n",
    "#combine the dataframes\n",
    "df_combined = pd.concat([df, df_forecast, df_actual])\n",
    "\n",
    "#convert new 'Date' column to datetime\n",
    "df_combined['Date'] = df_combined['Date'].dt.to_timestamp()\n",
    "df_actual['Date'] = df_actual['Date'].dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chart forecasted vs actual vs historical for all accidents\n",
    "chart = alt.Chart(df_combined).mark_line().encode(\n",
    "    x='Date:T', \n",
    "    y='Accidents:Q',  \n",
    "    color='Type:N',  \n",
    "    tooltip=['Date:T', 'Accidents:Q', 'Type:N']  \n",
    ").properties(\n",
    "    title=\"Accident Forecast vs Actual (2020) - Monthly Data\",\n",
    "    width=800\n",
    ")\n",
    "\n",
    "#scatter points for actual 2020 accidents\n",
    "scatter = alt.Chart(df_actual).mark_point(filled=True, size=10).encode(\n",
    "    x='Date:T',\n",
    "    y='Accidents:Q',\n",
    "    color=alt.value('blue'),\n",
    "    tooltip=['Date:T', 'Accidents:Q']\n",
    ")\n",
    "\n",
    "final_chart = chart + scatter\n",
    "\n",
    "final_chart.configure_view(\n",
    "    strokeWidth=0  \n",
    ")\n",
    "\n",
    "final_chart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeline for number of collisions (alcohol vs no alcohol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained on up to 2019, predicting 2020, but separating by alcohol vs. no alcohol and comparing to actuals. This will also be completed using a time series model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert collision date to year_month to group by every month from 2001 for alcohol and no alcohol\n",
    "dfa = dfc.copy()\n",
    "dfa[\"year_month\"] = dfa[\"collision_date\"].dt.to_period('M') \n",
    "monthly_alcohol_accidents = dfa[dfa['alcohol_involved'] == 1].groupby('year_month').size()\n",
    "monthly_no_alcohol_accidents = dfa[dfa['alcohol_involved'] == 0].groupby('year_month').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_alcohol_accidents.info()\n",
    "monthly_no_alcohol_accidents.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model for alcohol related accidents to forecast 2020\n",
    "forecast_2020_monthly_alcohol = fit_sarima_and_forecast(monthly_alcohol_accidents[:'2019-12'])\n",
    "actual_2020_monthly_alcohol = monthly_alcohol_accidents['2020-01':'2020-12']\n",
    "\n",
    "#fit model for non alcohol related accidents to forecast 2020\n",
    "forecast_2020_monthly_no_alcohol = fit_sarima_and_forecast(monthly_no_alcohol_accidents[:'2019-12'])\n",
    "actual_2020_monthly_no_alcohol = monthly_no_alcohol_accidents['2020-01':'2020-12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df for historical alcohol accidents\n",
    "df_alcohol = pd.DataFrame({\n",
    "    'Date': monthly_alcohol_accidents.index,\n",
    "    'Accidents': monthly_alcohol_accidents.values,\n",
    "    'Type': ['Historical'] * len(monthly_alcohol_accidents)\n",
    "})\n",
    "\n",
    "#forecasted 2020 data for alcohol accidents\n",
    "df_forecast_alcohol = pd.DataFrame({\n",
    "    'Date': forecast_2020_monthly_alcohol.index,\n",
    "    'Accidents': forecast_2020_monthly_alcohol.values,\n",
    "    'Type': ['Forecasted'] * len(forecast_2020_monthly_alcohol)\n",
    "})\n",
    "\n",
    "#actual 2020 data for alcohol accidents in 2020\n",
    "df_actual_alcohol = pd.DataFrame({\n",
    "    'Date': actual_2020_monthly_alcohol.index,\n",
    "    'Accidents': actual_2020_monthly_alcohol.values,\n",
    "    'Type': ['Actual'] * len(actual_2020_monthly_alcohol)\n",
    "})\n",
    "\n",
    "#combined df for all alcohol data\n",
    "df_combined_alcohol = pd.concat([df_alcohol, df_forecast_alcohol, df_actual_alcohol])\n",
    "\n",
    "#convert new 'Date' column to datetime\n",
    "df_combined_alcohol['Date'] = df_combined_alcohol['Date'].dt.to_timestamp()\n",
    "df_actual_alcohol['Date'] = df_actual_alcohol['Date'].dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chart for alcohol data\n",
    "chart_alcohol = alt.Chart(df_combined_alcohol).mark_line().encode(\n",
    "    x='Date:T',\n",
    "    y='Accidents:Q',\n",
    "    color='Type:N',\n",
    "    tooltip=['Date:T', 'Accidents:Q', 'Type:N']\n",
    ").properties(\n",
    "    title=\"Alcohol-related Accidents (2020): Forecasted vs. Actual\",\n",
    "    width=800\n",
    ")\n",
    "\n",
    "#scatter for actual 2020 data\n",
    "chart_actual_alcohol = alt.Chart(df_actual_alcohol).mark_point(filled=True, size=10).encode(\n",
    "    x='Date:T',\n",
    "    y='Accidents:Q',\n",
    "    color=alt.value('blue'),\n",
    "    tooltip=['Date:T', 'Accidents:Q']\n",
    ")\n",
    "\n",
    "chart_alcohol + chart_actual_alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df for histrorical non-alcohol accidents\n",
    "df_no_alcohol = pd.DataFrame({\n",
    "    'Date': monthly_no_alcohol_accidents.index,\n",
    "    'Accidents': monthly_no_alcohol_accidents.values,\n",
    "    'Type': ['Historical'] * len(monthly_no_alcohol_accidents)\n",
    "})\n",
    "\n",
    "#forecasted 2020 data for non-alcohol accidents\n",
    "df_forecast_no_alcohol = pd.DataFrame({\n",
    "    'Date': forecast_2020_monthly_no_alcohol.index,\n",
    "    'Accidents': forecast_2020_monthly_no_alcohol.values,\n",
    "    'Type': ['Forecasted'] * len(forecast_2020_monthly_no_alcohol)\n",
    "})\n",
    "\n",
    "#actual 2020 data for non-alcohol accidents \n",
    "df_actual_no_alcohol = pd.DataFrame({\n",
    "    'Date': actual_2020_monthly_no_alcohol.index,\n",
    "    'Accidents': actual_2020_monthly_no_alcohol.values,\n",
    "    'Type': ['Actual'] * len(actual_2020_monthly_no_alcohol)\n",
    "})\n",
    "\n",
    "#combine df for all no-alcohol data\n",
    "df_combined_no_alcohol = pd.concat([df_no_alcohol, df_forecast_no_alcohol, df_actual_no_alcohol])\n",
    "\n",
    "#convert new 'Date' column to datetime\n",
    "df_combined_no_alcohol['Date'] = df_combined_no_alcohol['Date'].dt.to_timestamp()\n",
    "df_actual_no_alcohol['Date'] = df_actual_no_alcohol['Date'].dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection using Random Forest feature importance against several labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrf = df.copy()\n",
    "\n",
    "dfrf[\"minute\"] = pd.to_datetime(df[\"collision_time\"], format = \"%H:%M:%S\").dt.minute\n",
    "dfrf[\"day\"] = pd.to_datetime(df[\"collision_time\"], format = \"%H:%M:%S\").dt.day_of_year\n",
    "\n",
    "\n",
    "drop_feats = [\"collision_severity\", \"killed_victims\", \"injured_victims\", \"severe_injury_count\",\n",
    "              \"other_visible_injury_count\", \"complaint_of_pain_injury_count\", \"pedestrian_killed_count\", \"pedestrian_injured_count\",\n",
    "              \"bicyclist_killed_count\", \"bicyclist_injured_count\", \"motorcyclist_killed_count\", \"motorcyclist_injured_count\",\n",
    "              \"case_id\", \"process_date\", \"hour\", \"collision_date\", \"process_date\", \"collision_time\",\n",
    "              \"city_division_lapd\", \"caltrans_county\", \"caltrans_district\", \"state_route\", \"postmile\"]\n",
    "\n",
    "dfnan = pd.DataFrame()\n",
    "dfnan[\"predictor\"] = (dfrf.isna().sum() / dfrf.isna().count()).sort_values().index\n",
    "dfnan[\"p_nan\"] = (dfrf.isna().sum() / dfrf.isna().count()).sort_values().values\n",
    "\n",
    "drop_nans = dfnan.query(\"p_nan > 0.8\")[\"predictor\"] # drop features that are more than 80 % nan\n",
    "\n",
    "X = dfrf.drop(drop_feats, axis = 1).drop(drop_nans, axis = 1).convert_dtypes()\n",
    "\n",
    "numcols = []\n",
    "for column in X:\n",
    "    if X[column].dtype != \"string[python]\":\n",
    "        numcols.append(column)\n",
    "badnumcols = [column for column in numcols if column not in [\"distance\", \"party_count\", \"latitude\", \"longitude\", \"year\", \"minute\", \"day\"]] # only keep these ones as numeric\n",
    "X[badnumcols] = X[badnumcols].astype(\"string[python]\")\n",
    "\n",
    "badcats = [column for column in X if X[column].nunique() > 100 and X[column].dtype == \"string[python]\"]\n",
    "X = X.drop(badcats, axis = 1) # drop categorical features with more than 100 unique groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding number of unique groups for categorical features\n",
    "\n",
    "strcolumns = []\n",
    "for column in X:\n",
    "    if X[column].dtype == \"string[python]\":\n",
    "        strcolumns.append(column)\n",
    "\n",
    "columns, uniques = [], []\n",
    "\n",
    "for column in strcolumns:\n",
    "    columns.append(column)\n",
    "    uniques.append(len(X[column].value_counts()))\n",
    "                   \n",
    "opdf = pd.DataFrame({\"column\": columns, \"unique\": uniques})\n",
    "\n",
    "uns = opdf.sort_values(\"unique\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xoh = pd.get_dummies(X)\n",
    "y = dfrf[\"injured_victims\"].fillna(0)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(Xoh, y, random_state = 13)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 10, random_state = 13)\n",
    "\n",
    "rff = rf.fit(X_tr, y_tr)\n",
    "\n",
    "rfpred = rff.predict(X_te)\n",
    "\n",
    "print(\"RMSE: %.3f\" % (np.sqrt(mean_squared_error(y_te, rfpred))))\n",
    "print(\"Proportion correct: %.3f \"% ((y_te == rfpred.astype(int)).mean()))\n",
    "\n",
    "pd.DataFrame({\"feature\": rff.feature_names_in_, \"importance\": rff.feature_importances_}).sort_values(\"importance\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xoh2 = pd.get_dummies(X.drop(\"party_count\", axis = 1))\n",
    "y2 = dfrf[\"injured_victims\"].fillna(0) / dfrf[\"party_count\"].fillna(1) # repeating this time using the injuries per party involved\n",
    "\n",
    "X2_tr, X2_te, y2_tr, y2_te = train_test_split(Xoh2, y2, random_state = 13)\n",
    "\n",
    "rf2 = RandomForestRegressor(n_estimators = 10, random_state = 13)\n",
    "\n",
    "rff2 = rf2.fit(X2_tr, y2_tr)\n",
    "\n",
    "rfpred2 = rff2.predict(X2_te)\n",
    "\n",
    "print(\"RMSE: %.3f\" % (np.sqrt(mean_squared_error(y2_te, rfpred2))))\n",
    "print(\"Proportion correct: %.3f \"% ((y2_te == rfpred2.astype(int)).mean()))\n",
    "\n",
    "pd.DataFrame({\"feature\": rff2.feature_names_in_, \"importance\": rff2.feature_importances_}).sort_values(\"importance\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xoh3 = pd.get_dummies(X)\n",
    "y3 = dfrf[\"collision_severity\"] # repeating this time using categorical label\n",
    "\n",
    "X3_tr, X3_te, y3_tr, y3_te = train_test_split(Xoh3, y3, random_state = 13)\n",
    "\n",
    "rf3 = RandomForestClassifier(n_estimators = 10, random_state = 13)\n",
    "\n",
    "rff3 = rf3.fit(X3_tr, y3_tr)\n",
    "\n",
    "rfpred3 = rff3.predict(X3_te)\n",
    "\n",
    "print(\"Proportion correct: %.3f \"% ((y3_te == rfpred3).mean()))\n",
    "\n",
    "cf3 = confusion_matrix(y3_te, rfpred3)\n",
    "\n",
    "cmp3 = ConfusionMatrixDisplay(confusion_matrix=cf3, display_labels=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cmp3.plot(ax=ax)\n",
    "\n",
    "pd.DataFrame({\"feature\": rff3.feature_names_in_, \"importance\": rff3.feature_importances_}).sort_values(\"importance\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection using LASSO on fitted GLM against several labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "Xoh = pd.get_dummies(X)\n",
    "\n",
    "# Impute missing values with the mean of each column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer to the data and transform it, then convert back to DataFrame\n",
    "Xoh_imputed = imputer.fit_transform(Xoh)\n",
    "Xoh_imputed = pd.DataFrame(Xoh_imputed, columns=Xoh.columns)\n",
    "\n",
    "# Define the target variable\n",
    "y = dfrf[\"injured_victims\"].fillna(0) \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(Xoh_imputed, y, random_state=13)\n",
    "\n",
    "# Initialize the Lasso model\n",
    "lasso = Lasso(alpha=0.1, random_state=13)\n",
    "\n",
    "# Fit the Lasso model\n",
    "lasso.fit(X_tr, y_tr)\n",
    "\n",
    "# Predict on the test set\n",
    "lasso_pred = lasso.predict(X_te)\n",
    "\n",
    "# Calculate and print RMSE\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_te, lasso_pred))\n",
    "print(f\"Lasso RMSE: {rmse_lasso:.3f}\")\n",
    "\n",
    "# Show the coefficients of the features to understand feature importance\n",
    "feature_importance_lasso = pd.DataFrame({\n",
    "    \"feature\": X_tr.columns, \n",
    "    \"coefficient\": np.abs(lasso.coef_)\n",
    "}).sort_values(\"coefficient\", ascending=False)\n",
    "\n",
    "# Print the top 20 most important features\n",
    "print(\"Top Features (by absolute coefficient value):\")\n",
    "print(feature_importance_lasso.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xoh2 = pd.get_dummies(X.drop(\"party_count\", axis = 1))\n",
    "y2 = dfrf[\"injured_victims\"].fillna(0) / dfrf[\"party_count\"].fillna(1)\n",
    "\n",
    "# Fit the imputer to the data and transform it, then convert back to DataFrame\n",
    "Xoh2_imputed = imputer.fit_transform(Xoh2)\n",
    "Xoh2_imputed = pd.DataFrame(Xoh2_imputed, columns=Xoh2.columns)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(Xoh2_imputed, y2, random_state=13)\n",
    "\n",
    "# Initialize the Lasso model\n",
    "lasso = Lasso(alpha=0.1, random_state=13)\n",
    "\n",
    "# Fit the Lasso model\n",
    "lasso.fit(X_tr, y_tr)\n",
    "\n",
    "# Predict on the test set\n",
    "lasso_pred = lasso.predict(X_te)\n",
    "\n",
    "# Calculate and print RMSE\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_te, lasso_pred))\n",
    "print(f\"Lasso RMSE: {rmse_lasso:.3f}\")\n",
    "\n",
    "# Show the coefficients of the features to understand feature importance\n",
    "feature_importance_lasso = pd.DataFrame({\n",
    "    \"feature\": X_tr.columns, \n",
    "    \"coefficient\": np.abs(lasso.coef_)\n",
    "}).sort_values(\"coefficient\", ascending=False)\n",
    "\n",
    "# Print the top 20 most important features\n",
    "print(\"Top Features (by absolute coefficient value):\")\n",
    "print(feature_importance_lasso.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

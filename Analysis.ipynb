{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import altair as alt\n",
    "import scipy.interpolate as interpolate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(\"switrs.sqlite\")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT * FROM collisions WHERE county_location = 'los angeles'\n",
    "    \"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, con, parse_dates = [\"collision_date\"])\n",
    "df[\"year\"] = df[\"collision_date\"].dt.year\n",
    "df[\"hour\"] = pd.to_datetime(df[\"collision_time\"]).dt.hour\n",
    "df = df.query(\"year < 2021\") # remove incomplete 2021 data\n",
    "df[\"alcohol_involved\"] = df[\"alcohol_involved\"].fillna(0) # convert NaN to 0 in alcohol use column\n",
    "\n",
    "dfc = df[[\"case_id\", \"county_location\", \"alcohol_involved\", \"collision_severity\", \"injured_victims\", \"collision_date\", \"year\", \"collision_time\", \"hour\", \"party_count\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of the proportion of crashes involving alcohol by time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will attempt to use multiple models to fit this distribution. We will use polynomial splines and B-splines but it appears a skewed-normal may be a possible fit as well.\n",
    "\n",
    "x = dfc.groupby(\"hour\")[\"hour\"].mean()\n",
    "ya = dfc[dfc[\"alcohol_involved\"] == 1].groupby(\"hour\")[\"alcohol_involved\"].count() / len(dfc[dfc[\"alcohol_involved\"] == 1])\n",
    "yn = dfc[dfc[\"alcohol_involved\"] == 0].groupby(\"hour\")[\"alcohol_involved\"].count() / len(dfc[dfc[\"alcohol_involved\"] == 0])\n",
    "adj_hour = [16, 17, 18, 19, 20, 21, 22, 23, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "adjlabel = \"{0: '8am', 1: '9am', 2: '10am', 3: '11am', 4: '12pm', 5: '1pm', 6: '2pm', 7: '3pm', 8: '4pm', 9: '5pm', 10: '6pm', 11: '7pm', 12: '8pm', 13: '9pm', 14: '10pm', 15: '11pm', 16: '12am', 17: '1am', 18: '2am', 19: '3am', 20: '4am', 21: '5am', 22: '6am', 23: '7am'}[datum.value]\"\n",
    "\n",
    "dfah = pd.DataFrame({\"hour\": x, \"adj_hour\": adj_hour, \"collisions_a\": ya, \"collisions_n\": yn})\n",
    "\n",
    "pla = alt.Chart(dfah).mark_bar(width = 20, color = \"orange\", opacity = 0.75).encode(\n",
    "    x = alt.X(\"adj_hour:O\", title = \"Hour of the day\", axis = alt.Axis(labelExpr = adjlabel)),\n",
    "    y = alt.Y(\"collisions_a\", title = \"Number of collisions (alcohol involved)\").stack(None)\n",
    ").properties(width = 600, height = 200)\n",
    "\n",
    "pln = alt.Chart(dfah).mark_bar(width = 20, opacity = 0.75).encode(\n",
    "    x = alt.X(\"adj_hour:O\", title = \"Hour of the day\", axis = alt.Axis(labelExpr = adjlabel)),\n",
    "    y = alt.Y(\"collisions_n\", title = \"Number of collisions (alcohol not involved)\").stack(None)\n",
    ").properties(width = 600, height = 200)\n",
    "\n",
    "(pln + pla).resolve_axis(y = \"independent\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dfc.groupby(\"hour\")[\"hour\"].mean()\n",
    "y = dfc.groupby(\"hour\")[\"alcohol_involved\"].mean()\n",
    "xx = np.linspace(x.min(), x.max(), 24)\n",
    "\n",
    "t, c, k = interpolate.splrep(x, y, k = 3, s = 0.0001)\n",
    "ypred = interpolate.splev(xx, (t, c, k))\n",
    "\n",
    "adj_hour = [16, 17, 18, 19, 20, 21, 22, 23, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "dfspl = pd.DataFrame({\"hour\": x, \"adj_hour\": adj_hour, \"p\": y, \"p_fit\": ypred})\n",
    "\n",
    "spl_rmse = mean_squared_error(dfspl[\"p\"], dfspl[\"p_fit\"])\n",
    "print(\"Root mean squared error: %.5f\" % (np.sqrt(spl_rmse)))\n",
    "\n",
    "adjlabel = \"{0: '8am', 1: '9am', 2: '10am', 3: '11am', 4: '12pm', 5: '1pm', 6: '2pm', 7: '3pm', 8: '4pm', 9: '5pm', 10: '6pm', 11: '7pm', 12: '8pm', 13: '9pm', 14: '10pm', 15: '11pm', 16: '12am', 17: '1am', 18: '2am', 19: '3am', 20: '4am', 21: '5am', 22: '6am', 23: '7am'}[datum.value]\"\n",
    "adjscale = alt.Scale(domain = [-0.5, 23.5])\n",
    "\n",
    "spl_base = alt.Chart(dfspl).mark_bar(width = 20, color = \"orange\").encode(\n",
    "    x = alt.X(\"adj_hour\", axis = alt.Axis(labelExpr = adjlabel)),\n",
    "    y = alt.Y(\"p\")\n",
    ").properties(width = 600, height = 200)\n",
    "\n",
    "spl = alt.Chart(dfspl).mark_line(color = \"red\").encode(\n",
    "    x = alt.X(\"adj_hour\", title = \"Hour of the day\", scale = adjscale),\n",
    "    y = alt.Y(\"p_fit\", title = \"Probability of alcohol involvement\")\n",
    ").properties(width = 600, height = 200)\n",
    "\n",
    "(spl_base + spl).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distributions of the number of injuries per collision, alcohol vs. no alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These appear to follow exponential or Weibull distributions. We will determine which is the best fit and then compare to see if alcohol has an effect on the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeline for number of collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on up to 2019, predict 2020, compare to actual. This will be done using a time series model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeline for number of collisions (alcohol vs no alcohol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained on up to 2019, predicting 2020, but separating by alcohol vs. no alcohol and comparing to actuals. This will also be completed using a time series model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection using LASSO on fitted GLM against several labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "Xoh = pd.get_dummies(X)\n",
    "\n",
    "# Impute missing values with the mean of each column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit the imputer to the data and transform it, then convert back to DataFrame\n",
    "Xoh_imputed = imputer.fit_transform(Xoh)\n",
    "Xoh_imputed = pd.DataFrame(Xoh_imputed, columns=Xoh.columns)\n",
    "\n",
    "# Define the target variable\n",
    "y = dfrf[\"injured_victims\"].fillna(0) \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(Xoh_imputed, y, random_state=13)\n",
    "\n",
    "# Initialize the Lasso model\n",
    "lasso = Lasso(alpha=0.1, random_state=13)\n",
    "\n",
    "# Fit the Lasso model\n",
    "lasso.fit(X_tr, y_tr)\n",
    "\n",
    "# Predict on the test set\n",
    "lasso_pred = lasso.predict(X_te)\n",
    "\n",
    "# Calculate and print RMSE\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_te, lasso_pred))\n",
    "print(f\"Lasso RMSE: {rmse_lasso:.3f}\")\n",
    "\n",
    "# Show the coefficients of the features to understand feature importance\n",
    "feature_importance_lasso = pd.DataFrame({\n",
    "    \"feature\": X_tr.columns, \n",
    "    \"coefficient\": np.abs(lasso.coef_)\n",
    "}).sort_values(\"coefficient\", ascending=False)\n",
    "\n",
    "# Print the top 20 most important features\n",
    "print(\"Top Features (by absolute coefficient value):\")\n",
    "print(feature_importance_lasso.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xoh2 = pd.get_dummies(X.drop(\"party_count\", axis = 1))\n",
    "y2 = dfrf[\"injured_victims\"].fillna(0) / dfrf[\"party_count\"].fillna(1)\n",
    "\n",
    "# Fit the imputer to the data and transform it, then convert back to DataFrame\n",
    "Xoh2_imputed = imputer.fit_transform(Xoh2)\n",
    "Xoh2_imputed = pd.DataFrame(Xoh2_imputed, columns=Xoh2.columns)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(Xoh2_imputed, y2, random_state=13)\n",
    "\n",
    "# Initialize the Lasso model\n",
    "lasso = Lasso(alpha=0.1, random_state=13)\n",
    "\n",
    "# Fit the Lasso model\n",
    "lasso.fit(X_tr, y_tr)\n",
    "\n",
    "# Predict on the test set\n",
    "lasso_pred = lasso.predict(X_te)\n",
    "\n",
    "# Calculate and print RMSE\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_te, lasso_pred))\n",
    "print(f\"Lasso RMSE: {rmse_lasso:.3f}\")\n",
    "\n",
    "# Show the coefficients of the features to understand feature importance\n",
    "feature_importance_lasso = pd.DataFrame({\n",
    "    \"feature\": X_tr.columns, \n",
    "    \"coefficient\": np.abs(lasso.coef_)\n",
    "}).sort_values(\"coefficient\", ascending=False)\n",
    "\n",
    "# Print the top 20 most important features\n",
    "print(\"Top Features (by absolute coefficient value):\")\n",
    "print(feature_importance_lasso.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature selection using Random Forest feature importance against several labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfrf = df.copy()\n",
    "\n",
    "dfrf[\"minute\"] = pd.to_datetime(df[\"collision_time\"], format = \"%H:%M:%S\").dt.minute\n",
    "dfrf[\"day\"] = pd.to_datetime(df[\"collision_time\"], format = \"%H:%M:%S\").dt.day_of_year\n",
    "\n",
    "\n",
    "drop_feats = [\"collision_severity\", \"killed_victims\", \"injured_victims\", \"severe_injury_count\",\n",
    "              \"other_visible_injury_count\", \"complaint_of_pain_injury_count\", \"pedestrian_killed_count\", \"pedestrian_injured_count\",\n",
    "              \"bicyclist_killed_count\", \"bicyclist_injured_count\", \"motorcyclist_killed_count\", \"motorcyclist_injured_count\",\n",
    "              \"case_id\", \"process_date\", \"hour\", \"collision_date\", \"process_date\", \"collision_time\",\n",
    "              \"city_division_lapd\", \"caltrans_county\", \"caltrans_district\", \"state_route\", \"postmile\"]\n",
    "\n",
    "dfnan = pd.DataFrame()\n",
    "dfnan[\"predictor\"] = (dfrf.isna().sum() / dfrf.isna().count()).sort_values().index\n",
    "dfnan[\"p_nan\"] = (dfrf.isna().sum() / dfrf.isna().count()).sort_values().values\n",
    "\n",
    "drop_nans = dfnan.query(\"p_nan > 0.8\")[\"predictor\"] # drop features that are more than 80 % nan\n",
    "\n",
    "X = dfrf.drop(drop_feats, axis = 1).drop(drop_nans, axis = 1).convert_dtypes()\n",
    "\n",
    "numcols = []\n",
    "for column in X:\n",
    "    if X[column].dtype != \"string[python]\":\n",
    "        numcols.append(column)\n",
    "badnumcols = [column for column in numcols if column not in [\"distance\", \"party_count\", \"latitude\", \"longitude\", \"year\", \"minute\", \"day\"]] # only keep these ones as numeric\n",
    "X[badnumcols] = X[badnumcols].astype(\"string[python]\")\n",
    "\n",
    "badcats = [column for column in X if X[column].nunique() > 100 and X[column].dtype == \"string[python]\"]\n",
    "X = X.drop(badcats, axis = 1) # drop categorical features with more than 100 unique groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding number of unique groups for categorical features\n",
    "\n",
    "strcolumns = []\n",
    "for column in X:\n",
    "    if X[column].dtype == \"string[python]\":\n",
    "        strcolumns.append(column)\n",
    "\n",
    "columns, uniques = [], []\n",
    "\n",
    "for column in strcolumns:\n",
    "    columns.append(column)\n",
    "    uniques.append(len(X[column].value_counts()))\n",
    "                   \n",
    "opdf = pd.DataFrame({\"column\": columns, \"unique\": uniques})\n",
    "\n",
    "uns = opdf.sort_values(\"unique\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xoh = pd.get_dummies(X)\n",
    "y = dfrf[\"injured_victims\"].fillna(0)\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(Xoh, y, random_state = 13)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 10, random_state = 13)\n",
    "\n",
    "rff = rf.fit(X_tr, y_tr)\n",
    "\n",
    "rfpred = rff.predict(X_te)\n",
    "\n",
    "print(\"RMSE: %.3f\" % (np.sqrt(mean_squared_error(y_te, rfpred))))\n",
    "print(\"Proportion correct: %.3f \"% ((y_te == rfpred.astype(int)).mean()))\n",
    "\n",
    "pd.DataFrame({\"feature\": rff.feature_names_in_, \"importance\": rff.feature_importances_}).sort_values(\"importance\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xoh2 = pd.get_dummies(X.drop(\"party_count\", axis = 1))\n",
    "y2 = dfrf[\"injured_victims\"].fillna(0) / dfrf[\"party_count\"].fillna(1) # repeating this time using the injuries per party involved\n",
    "\n",
    "X2_tr, X2_te, y2_tr, y2_te = train_test_split(Xoh2, y2, random_state = 13)\n",
    "\n",
    "rf2 = RandomForestRegressor(n_estimators = 10, random_state = 13)\n",
    "\n",
    "rff2 = rf2.fit(X2_tr, y2_tr)\n",
    "\n",
    "rfpred2 = rff2.predict(X2_te)\n",
    "\n",
    "print(\"RMSE: %.3f\" % (np.sqrt(mean_squared_error(y2_te, rfpred2))))\n",
    "print(\"Proportion correct: %.3f \"% ((y2_te == rfpred2.astype(int)).mean()))\n",
    "\n",
    "pd.DataFrame({\"feature\": rff2.feature_names_in_, \"importance\": rff2.feature_importances_}).sort_values(\"importance\", ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xoh3 = pd.get_dummies(X)\n",
    "y3 = dfrf[\"collision_severity\"] # repeating this time using categorical label\n",
    "\n",
    "X3_tr, X3_te, y3_tr, y3_te = train_test_split(Xoh3, y3, random_state = 13)\n",
    "\n",
    "rf3 = RandomForestClassifier(n_estimators = 10, random_state = 13)\n",
    "\n",
    "rff3 = rf3.fit(X3_tr, y3_tr)\n",
    "\n",
    "rfpred3 = rff3.predict(X3_te)\n",
    "\n",
    "print(\"Proportion correct: %.3f \"% ((y3_te == rfpred3).mean()))\n",
    "\n",
    "cf3 = confusion_matrix(y3_te, rfpred3)\n",
    "\n",
    "cmp3 = ConfusionMatrixDisplay(confusion_matrix=cf3, display_labels=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cmp3.plot(ax=ax)\n",
    "\n",
    "pd.DataFrame({\"feature\": rff3.feature_names_in_, \"importance\": rff3.feature_importances_}).sort_values(\"importance\", ascending = False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
